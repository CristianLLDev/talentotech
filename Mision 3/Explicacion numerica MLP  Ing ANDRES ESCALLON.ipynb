{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP: Perceptrón Multicapa, la Base de las Redes Neuronales Artificiales\n",
    "Un Perceptrón Multicapa (MLP, por sus siglas en inglés) es un tipo fundamental de red neuronal artificial. Imagina una red de neuronas interconectadas, organizadas en capas, donde cada neurona procesa información y la pasa a la siguiente capa. Esta estructura le permite a la red aprender patrones complejos y realizar tareas de clasificación y regresión.\n",
    "\n",
    "¿Cómo funciona un MLP?\n",
    "Capas:\n",
    "\n",
    "Capa de entrada: Recibe los datos de entrada.\n",
    "Capas ocultas: Realizan los cálculos internos y extraen características de los datos.\n",
    "Capa de salida: Produce la salida final, que puede ser una clasificación (por ejemplo, si es un gato o un perro) o un valor numérico (por ejemplo, la temperatura).\n",
    "Neuronas:\n",
    "\n",
    "Cada neurona realiza una operación simple: suma ponderada de sus entradas y aplica una función de activación.\n",
    "La función de activación introduce no linealidad a la red, permitiendo aprender patrones más complejos.\n",
    "Entrenamiento:\n",
    "\n",
    "El MLP se entrena con un conjunto de datos de entrada y sus correspondientes salidas correctas.\n",
    "A través de un proceso llamado retropropagación, el modelo ajusta los pesos de las conexiones entre las neuronas para minimizar el error entre las predicciones y las etiquetas reales.\n",
    "¿Por qué son importantes los MLP?\n",
    "Versatilidad: Pueden resolver una amplia variedad de problemas, desde clasificación hasta regresión y generación de datos.\n",
    "Aprendizaje automático: Pueden aprender patrones complejos a partir de datos sin ser programados explícitamente.\n",
    "Adaptabilidad: Se pueden ajustar a nuevos datos y mejorar su rendimiento con más entrenamiento.\n",
    "¿Dónde se utilizan los MLP?\n",
    "Reconocimiento de imágenes: Identificar objetos en imágenes.\n",
    "Procesamiento de lenguaje natural: Análisis de sentimientos, traducción automática.\n",
    "Series temporales: Predicción de valores futuros.\n",
    "Y muchas más...\n",
    "Ventajas de los MLP:\n",
    "Poder de representación: Pueden aprender funciones muy complejas.\n",
    "Aprendizaje supervisado: Pueden aprender de datos etiquetados.\n",
    "Fácil implementación: Existen muchas librerías que facilitan su uso.\n",
    "Desventajas de los MLP:\n",
    "Sobreajuste: Pueden memorizar los datos de entrenamiento en lugar de generalizar a nuevos datos.\n",
    "Tiempo de entrenamiento: Pueden requerir mucho tiempo para entrenar en grandes conjuntos de datos.\n",
    "Selección de hiperparámetros: La elección del número de capas, neuronas y otros parámetros puede ser compleja.\n",
    "En resumen, los MLP son una herramienta fundamental en el campo del aprendizaje automático. Su capacidad para aprender patrones complejos y adaptarse a nuevos datos los convierte en una elección popular para una amplia gama de aplicaciones.\n",
    "\n",
    "¿Te gustaría profundizar en algún aspecto específico de los MLP, como la función de activación, la retropropagación o su implementación en una librería como TensorFlow o PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " W = \n",
      " [[ 1  1]\n",
      " [-1  1]]\n",
      "\n",
      " X = \n",
      " [[1 5 2]\n",
      " [2 4 2]]\n",
      "\n",
      " WX_1 = \n",
      " [[ 3  9  4]\n",
      " [ 1 -1  0]]\n",
      "\n",
      " WX_2 = \n",
      " [[ 3  9  4]\n",
      " [ 1 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array_1 = np.array(     [ [1, 1], [-1,1] ]   )\n",
    "\n",
    "\n",
    "W = np.array(     [ [1, 1], [-1,1] ]   )\n",
    "print(\"\\n W = \\n\", W)\n",
    "#print()\n",
    "\n",
    "\n",
    "array_2 = np.array (   [  [1, 5, 2],    [2, 4, 2] ]    )\n",
    "\n",
    "X = np.array (   [  [1, 5, 2],    [2, 4, 2] ]    )\n",
    "print(\"\\n X = \\n\",X)\n",
    "#print()\n",
    "\n",
    "WX_1 = np.dot(W,X)\n",
    "print(\"\\n WX_1 = \\n\",WX_1)\n",
    "#print()\n",
    "\n",
    "WX_2 = np.matmul(W,X)\n",
    "print(\"\\n WX_2 = \\n\",WX_2)\n",
    "#print()\n",
    "\n",
    "#XW = np.matmul(X,W)\n",
    "#print(XW)\n",
    "#print()\n",
    "# Lo anterior no funciona por lo siguiente:\n",
    "#X_(2,3) por W_(2,2) = Resultado_(2,2)?\n",
    "#porque el numero de columnas de X es diferente al numero de filas de W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W_(2,2) por X_(2,3) = WX_(2,3)\n",
    "\n",
    "W*x + b = Z ---> Activation = output\n",
    "\n",
    "W_(3,4) por x_(4,5) = Z_(3,5)\n",
    "\n",
    "Asumiendo que todos los biases son cero y que NO hay función de activación (es decir: se deja pasar la misma información)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  0 -1]\n",
      " [-1  1  2 -3]\n",
      " [-1  1  5  0]]\n",
      "\n",
      "[[ 1  5  2 -1  0]\n",
      " [ 2  4  2  1 -1]\n",
      " [ 7  5  6 -1  1]\n",
      " [-8  0  4 -1  3]]\n",
      "\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[11.  9.  0.  1. -4.]\n",
      " [39.  9.  0.  3. -8.]\n",
      " [36. 24. 30. -3.  4.]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([   [1,1,0,-1], [-1,1,2,-3], [-1,1,5,0]  ])\n",
    "print(W)\n",
    "\n",
    "print()\n",
    "\n",
    "x = np.array([ [1,5,2,-1,0], [2,4,2,1,-1], [7,5,6,-1,1], [-8,0,4,-1,3] ])\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "b = np.zeros((3, 5)) # suponer todos los valores de los biases en cero\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "Z = np.matmul(W,x) + b\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación manual:\n",
    "W =\n",
    "[[ 1  1]\n",
    " [-1  1]]\n",
    "\n",
    "x=\n",
    " [[1 5 2]\n",
    " [2 4 2]]\n",
    "\n",
    "Entonces:\n",
    "\n",
    "W*x + b:\n",
    "\n",
    "b=zeros\n",
    "W*x  + 0 = W*x = Z\n",
    "[[ 3  9  4]\n",
    " [ 1 -1  0]]\n",
    "\n",
    "Z-->relu-->A\n",
    "A=\n",
    "[[ 3  9  4]\n",
    " [ 1  0  0]]\n",
    "\n",
    "===============\n",
    "W*x =\n",
    "[[ 3  9  4]\n",
    " [ 1 -1  0]]\n",
    "\n",
    "+ b (b=ones)\n",
    "W*x + b:\n",
    "[[ 4  10  5]\n",
    " [ 2  0   1]]\n",
    "\n",
    "W*x + b = Z\n",
    "Z-->relu-->A\n",
    "\n",
    "A=\n",
    "[[ 4  10  5]\n",
    " [ 2  0   1]]\n",
    "\n",
    "=========================\n",
    "\n",
    "Comparar el resultado conocido (Supervisado)\n",
    "Supongamos que el resultado conocido es:\n",
    "Y_real=\n",
    "[[ 4  10  5]\n",
    " [ 2  0   0]]\n",
    "y sucedió que la red calculó y dio:\n",
    "A=Y_estimado\n",
    "[[ 4  10  5]\n",
    " [ 2  0   0]]\n",
    "\n",
    "Error(Y_real-Y_estimada)=?\n",
    "E=\n",
    "[[ 0  0  0]\n",
    " [ 0  0   0]]\n",
    "\n",
    "===============================================\n",
    "O supongamos que el resultado conocido debe ser:\n",
    "Y_real=\n",
    "[[ -3  6  2]\n",
    " [ 0   5 -1]]\n",
    "y sucedió que la red calculó y dio:\n",
    "\n",
    "A=Y_estimado\n",
    "[[ 4  10  5]\n",
    " [ 2  0   0]]\n",
    "\n",
    "Error(Y_real-Y_estimada)=?\n",
    "E=\n",
    "[[ -7  -4  -3]\n",
    " [ -2  5   -1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOMEWORK:\n",
    "Explorar la pagína de tensorflow playground:\n",
    "\n",
    "https://playground.tensorflow.org/\n",
    "\n",
    "Ejemplo académico:\n",
    "Definiendo funciones en python para calcular:\n",
    "\n",
    "W x + b = Z --> relu ---> y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_from_MLP_input(W,x,b,activation=\"linear\"):\n",
    "  #print(\"W = \",end='\\n')\n",
    "  #print(W)\n",
    "  print(f\"W = \\n {W} \\n\")\n",
    "  #print()\n",
    "\n",
    "  print(\"x = \",end='\\n')\n",
    "  print(x)\n",
    "  print(\"b = \",end='\\n')\n",
    "  print(b)\n",
    "  # arguments:\n",
    "  # W,x and b are arrays\n",
    "\n",
    "  # operations\n",
    "  # Z = W x  +  b\n",
    "  #Z = np.matmul(W,x) + b\n",
    "  Z = np.dot(W,x) + b\n",
    "\n",
    "  print(\"Z = \",end='\\n')\n",
    "  print(Z)\n",
    "\n",
    "  if activation==\"linear\":\n",
    "    y = Z\n",
    "\n",
    "  if activation==\"relu\":\n",
    "    # Ternary operator to define a relu function\n",
    "    #y = Z if Z>=0 else 0\n",
    "    #y = Z if Z>0 else 0\n",
    "    #y = 0 if Z<0 else Z\n",
    "    #y = 0 if Z<=0 else Z\n",
    "    y = np.array([0 if z<=0 else z for z in Z])\n",
    "\n",
    "  #TO-DO:\n",
    "  \"\"\"\n",
    "  if activation==\"sigmoid\":\n",
    "    #\n",
    "  if activation==\"tanh\":\n",
    "    #\n",
    "  # Use many others\n",
    "  if activation==\"\":\n",
    "    #\n",
    "  if activation==\"\":\n",
    "    #\n",
    "  ...\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"y = \",end='\\n')\n",
    "  print(y)\n",
    "\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = \n",
      " [[ 1 -1  1]\n",
      " [ 1  1  0]\n",
      " [ 0  1  1]\n",
      " [ 1  0  1]] \n",
      "\n",
      "x = \n",
      "[2 1 3]\n",
      "b = \n",
      "[-5  0  1 -2]\n",
      "Z = \n",
      "[-1  3  5  3]\n",
      "y = \n",
      "[-1  3  5  3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1,  3,  5,  3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing (linear)\n",
    "\n",
    "W = np.array([ [1,-1,1]  , [1,1,0] ,  [0,1,1] ,  [1,0,1]    ])\n",
    "x = np.array([2, 1, 3])\n",
    "b = np.array([-5, 0, 1, -2])\n",
    "\n",
    "y = get_output_from_MLP_input(W,x,b)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = \n",
      " [[ 1 -1  1]\n",
      " [ 1  1  0]\n",
      " [ 0  1  1]\n",
      " [ 1  0  1]] \n",
      "\n",
      "x = \n",
      "[2 1 3]\n",
      "b = \n",
      "[-5  0  1 -2]\n",
      "Z = \n",
      "[-1  3  5  3]\n",
      "y = \n",
      "[-1  3  5  3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1,  3,  5,  3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing (linear)\n",
    "\n",
    "W = np.array([ [1,-1,1]  , [1,1,0] ,  [0,1,1] ,  [1,0,1]    ])\n",
    "x = np.array([2, 1, 3])\n",
    "b = np.array([-5, 0, 1, -2])\n",
    "\n",
    "y = get_output_from_MLP_input(W,x,b, activation=\"linear\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = \n",
      " [[ 1 -1  1]\n",
      " [ 1  1  0]\n",
      " [ 0  1  1]\n",
      " [ 1  0  1]] \n",
      "\n",
      "x = \n",
      "[2 1 3]\n",
      "b = \n",
      "[-5  0  1 -2]\n",
      "Z = \n",
      "[-1  3  5  3]\n",
      "y = \n",
      "[0 3 5 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 3, 5, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Testing (relu)\n",
    "\n",
    "W = np.array([ [1,-1,1]  , [1,1,0] ,  [0,1,1] ,  [1,0,1]    ])\n",
    "x = np.array([2, 1, 3])\n",
    "b = np.array([-5, 0, 1, -2])\n",
    "\n",
    "y = get_output_from_MLP_input(W,x,b,\"relu\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la multiplicacion entre matrices en general NO es conmutativa\n",
    "AB=C en general es distinto de BA=D\n",
    "\n",
    "(suponiendo que por las dimensiones se puedan operar)\n",
    "\n",
    "Ejemplo: al intercambiar Wx con xW hay error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = \n",
      " [2 1 3] \n",
      "\n",
      "x = \n",
      "[[ 1 -1  1]\n",
      " [ 1  1  0]\n",
      " [ 0  1  1]\n",
      " [ 1  0  1]]\n",
      "b = \n",
      "[-5  0  1 -2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (4,3) not aligned: 3 (dim 0) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mget_output_from_MLP_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m, in \u001b[0;36mget_output_from_MLP_input\u001b[1;34m(W, x, b, activation)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# arguments:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# W,x and b are arrays\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# operations\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Z = W x  +  b\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Z = np.matmul(W,x) + b\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ = \u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(Z)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,) and (4,3) not aligned: 3 (dim 0) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "y = get_output_from_MLP_input(x,W,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
